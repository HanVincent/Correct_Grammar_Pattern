{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.syntax import *\n",
    "from utils.counts import *\n",
    "# from utils.parse import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    \n",
    "def construct(block):\n",
    "    lines = [line for line in block.split('\\n') if line]\n",
    "    parsed = [ DotDict() for _ in range(len(lines)) ]\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        index, token, lemma, dep, tag, pos, head, children = line.split('\\t')\n",
    "\n",
    "        parsed[i].update({\n",
    "            'i': int(index),\n",
    "            'text': token,\n",
    "            'lemma_': lemma,\n",
    "            'dep_': dep,\n",
    "            'pos_': pos,\n",
    "            'tag_': tag,\n",
    "            'head': parsed[int(head)],\n",
    "            'children': [parsed[int(ch)] for ch in children.split(',') if ch],\n",
    "            'doc': parsed\n",
    "        })\n",
    "        \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block = '''0\twe\t-PRON-\tnsubj\tPRP\tPRON\t1\t\n",
    "# 1\twant\twant\tROOT\tVBP\tVERB\t1\t0,3\n",
    "# 2\tto\tto\taux\tTO\tPART\t3\t\n",
    "# 3\tdiscuss\tdiscuss\txcomp\tVB\tVERB\t1\t2,4\n",
    "# 4\tsomething\tsomething\tdobj\tNN\tNOUN\t3\t'''\n",
    "\n",
    "# entry = construct(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "fs = gzip.open('../coca.spacy.dep.txt.gz', 'rt', encoding='utf8')\n",
    "contents = fs.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = defaultdict(lambda: defaultdict(Counter))\n",
    "ngrams = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))\n",
    "sents = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))\n",
    "    \n",
    "def retrieve_dep(entry):\n",
    "    sent = ' '.join([tk.text for tk in entry])\n",
    "\n",
    "    if len(entry) > 30: return # skip long sentence\n",
    "    if '@@@' in sent: return\n",
    "\n",
    "    sent_score = score(sent)\n",
    "    \n",
    "    for token in entry:\n",
    "        if token.tag_ in POS['VERB']: # or tag == VERB\n",
    "            ptn_tks, ngram_tks = dep_to_ptns_ngrams(token)\n",
    "            ptn, ngram = ' '.join(ptn_tks), ' '.join(ngram_tks)\n",
    "\n",
    "            patterns[token.lemma_][token.dep_][ptn] += 1\n",
    "            ngrams[token.lemma_][token.dep_][ptn].append(ngram)\n",
    "            sents[token.lemma_][token.dep_][ptn].append((sent, sent_score))\n",
    "            \n",
    "        elif token.tag_ in POS['ADJ']:\n",
    "            pass\n",
    "        elif token.tag_ in POS['NOUN']:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in tqdm(contents):\n",
    "    retrieve_dep(construct(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize Ngrams and Sentences\n",
    "* ngrams count < 10\n",
    "* top 100 common sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_patterns = defaultdict(lambda: defaultdict(Counter))\n",
    "slim_ngrams = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))\n",
    "slim_sents = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))\n",
    "\n",
    "for word in patterns:\n",
    "    for dep in patterns[word]:\n",
    "        \n",
    "        truncated_ptns = truncate_k(patterns[word][dep], 1) # remove pattern whose count <= ?\n",
    "        \n",
    "        for ptn, cnt in truncated_ptns.items():\n",
    "            slim_patterns[word][dep][ptn] = cnt\n",
    "            \n",
    "            slim_ngrams[word][dep][ptn].extend(ngrams[word][dep][ptn])\n",
    "            \n",
    "            sorted_sents = sort_dict(sents[word][dep][ptn])\n",
    "            slim_sents[word][dep][ptn].extend([s for (s, sent_score) in sorted_sents[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store in sqlite or json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('static/data/coca.patterns.slim.json', 'w', encoding='utf8') as ws:\n",
    "#     json.dump({ 'patterns': slim_patterns, 'ngrams': slim_ngrams, 'sents': slim_sents }, ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('static/data/rules.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('DROP TABLE IF EXISTS rules;')\n",
    "\n",
    "cursor.execute('''CREATE TABLE rules \n",
    "(word NCHAR, dep CHARACTER, ptn NCHAR, norm_ptn NCHAR, count INTEGER, ngrams TEXT, sentences TEXT, \n",
    "PRIMARY KEY(word, dep, ptn));''')\n",
    "    \n",
    "for headword in slim_patterns:\n",
    "    for dep in slim_patterns[headword]:\n",
    "        for ptn in slim_patterns[headword][dep]:\n",
    "            count = slim_patterns[headword][dep][ptn]\n",
    "            \n",
    "            cursor.execute(\"INSERT INTO rules VALUES (?, ?, ?, ?, ?, ?, ?);\", \n",
    "                           (headword, dep, ptn, normalize(ptn), count, \n",
    "                            json.dumps(slim_ngrams[headword][dep][ptn]), \n",
    "                            json.dumps(slim_sents[headword][dep][ptn])))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
