{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.before_after import *\n",
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "from utils import explacy # https://github.com/tylerneylon/explacy\n",
    "# explacy.print_parse_info(nlp, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg') # ('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse BNC 所有句子，再取出有 discuss\n",
    "* read all lines\n",
    "* strip() and lower()\n",
    "* filter out target sentences\n",
    "* ** Use list() to keep filter generator **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_WORD = 'discuss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnc_remains = filter(lambda line: any([tk.lemma_ == TARGET_WORD for tk in line]), bnc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(file, reserved=None):\n",
    "    lines = map(lambda line: line.strip().lower(), open(file, 'r', encoding='utf8'))\n",
    "    remains = filter(lambda line: reserved in line, lines) if reserved else lines\n",
    "    return list(remains)\n",
    "\n",
    "bnc_temp = clean_data('../dataset/bnc.txt', TARGET_WORD)\n",
    "bnc_remains = list(map(lambda line: nlp(line), bnc_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 從 EF 中取得有 discuss 的句子，並且 parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef_temp = clean_data('../dataset/efcamp/ef.diff.simplize.despace.txt', 'discuss')\n",
    "# ef_remains = []\n",
    "# for index, line in enumerate(ef_temp):\n",
    "#     tokens = line.split(' ')\n",
    "    \n",
    "#     aft_sent = nlp(' '.join(to_after(tokens)))\n",
    "#     bef_sent = nlp(' '.join(to_before(tokens)))\n",
    "#     has_edit = any([tk[:2] in ['[-', '{+'] and TARGET_WORD in tokens[i-1] \n",
    "#                     for i, tk in enumerate(tokens)]) \n",
    "    \n",
    "#     ef_remains.append({\n",
    "#         'origin': line,\n",
    "#         'bef_sent': bef_sent,\n",
    "#         'aft_sent': aft_sent, \n",
    "#         'has_edit': has_edit\n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 dependency 抓 pattern\n",
    "#### 第一層 dependency\n",
    "dobj, prep, nsubj, nsubjpass, ccomp, xcomp, csubj, csubjpass, prt, acomp, oprd\n",
    "\n",
    "#### 第二層 dependency\n",
    "prep -> pobj, pcomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Head word only VERBS\n",
    "    # headword, patterns, dep, ngrams\n",
    "    patterns = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))\n",
    "    sents = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: []))) # for debug\n",
    "\n",
    "    for line in bnc_remains:\n",
    "        for tk in line:\n",
    "            if tk.tag_ in VERBS: \n",
    "                ptn, ngram = dep_to_pattern(tk)\n",
    "\n",
    "                patterns[tk.lemma_][tk.dep_][ptn].append(ngram)\n",
    "                sents[tk.lemma_][tk.dep_][ptn].append(tk.doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns['discuss']\n",
    "# sents['conj']['V-ed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree      Token    Dep type Lemma    Tag Part of Sp\n",
      "───────────── ──────── ──────── ──────── ─── ──────────\n",
      "          ┌─> I        nsubj    -PRON-   PRP PRON      \n",
      "┌┬────────┴── like     ROOT     like     VBP VERB      \n",
      "││        ┌─> the      det      the      DT  DET       \n",
      "│└─>┌─────┴── girl     dobj     girl     NN  NOUN      \n",
      "│   │     ┌─> who      nsubj    who      WP  NOUN      \n",
      "│   └─>┌──┴── takes    relcl    take     VBZ VERB      \n",
      "│      │  ┌─> an       det      an       DT  DET       \n",
      "│      └─>└── umbrella dobj     umbrella NN  NOUN      \n",
      "└───────────> .        punct    .        .   PUNCT     \n",
      "Dep tree       Token   Dep type Lemma   Tag Part of Sp\n",
      "────────────── ─────── ──────── ─────── ─── ──────────\n",
      "          ┌──> I       nsubj    -PRON-  PRP PRON      \n",
      "          │┌─> would   aux      would   MD  VERB      \n",
      "┌┬────────┴┴── like    ROOT     like    VB  VERB      \n",
      "││         ┌─> to      aux      to      TO  PART      \n",
      "│└─>┌┬─────┴── discuss xcomp    discuss VB  VERB      \n",
      "│   ││     ┌─> the     det      the     DT  DET       \n",
      "│   │└─>┌──┴── problem dobj     problem NN  NOUN      \n",
      "│   │   │  ┌─> you     nsubj    -PRON-  PRP PRON      \n",
      "│   │   └─>└── like    relcl    like    VBP VERB      \n",
      "│   └─────>┌── on      prep     on      IN  ADP       \n",
      "│          └─> Monday  pobj     monday  NNP PROPN     \n",
      "└────────────> .       punct    .       .   PUNCT     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "explacy.print_parse_info(nlp, 'I like the girl who takes an umbrella .')\n",
    "explacy.print_parse_info(nlp, 'I would like to discuss the problem you like on Monday.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
