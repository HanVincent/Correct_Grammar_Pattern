{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.before_after import *\n",
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "# import utils.explacy # https://github.com/tylerneylon/explacy\n",
    "# explacy.print_parse_info(nlp, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg') # ('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse BNC 所有句子，再取出有 discuss\n",
    "* read all lines\n",
    "* strip() and lower()\n",
    "* filter out target sentences\n",
    "* ** Use list() to keep filter generator **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_WORD = 'discuss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnc_remains = filter(lambda line: any([tk.lemma_ == TARGET_WORD for tk in line]), bnc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(file, reserved=None):\n",
    "    lines = map(lambda line: line.strip().lower(), open(file, 'r', encoding='utf8'))\n",
    "    remains = filter(lambda line: reserved in line, lines) if reserved else lines\n",
    "    return list(remains)\n",
    "\n",
    "bnc_temp = clean_data('../dataset/bnc.txt', TARGET_WORD)\n",
    "bnc_remains = list(map(lambda line: nlp(line), bnc_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 從 EF 中取得有 discuss 的句子，並且 parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef_temp = clean_data('../dataset/efcamp/ef.diff.simplize.despace.txt', 'discuss')\n",
    "# ef_remains = []\n",
    "# for index, line in enumerate(ef_temp):\n",
    "#     tokens = line.split(' ')\n",
    "    \n",
    "#     aft_sent = nlp(' '.join(to_after(tokens)))\n",
    "#     bef_sent = nlp(' '.join(to_before(tokens)))\n",
    "#     has_edit = any([tk[:2] in ['[-', '{+'] and TARGET_WORD in tokens[i-1] \n",
    "#                     for i, tk in enumerate(tokens)]) \n",
    "    \n",
    "#     ef_remains.append({\n",
    "#         'origin': line,\n",
    "#         'bef_sent': bef_sent,\n",
    "#         'aft_sent': aft_sent, \n",
    "#         'has_edit': has_edit\n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 dependency 抓 pattern\n",
    "#### 第一層 dependency\n",
    "dobj, prep, nsubj, nsubjpass, ccomp, xcomp, csubj, csubjpass, prt, acomp, oprd\n",
    "\n",
    "#### 第二層 dependency\n",
    "prep -> pobj, pcomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Head word only VERBS\n",
    "    # headword, patterns, dep, ngrams\n",
    "    patterns = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [])))\n",
    "    sents = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: []))) # for debug\n",
    "\n",
    "    for line in bnc_remains:\n",
    "        for tk in line:\n",
    "            if tk.tag_ in VERBS: \n",
    "                ptn, ngram = dep_to_pattern(tk)\n",
    "\n",
    "                patterns[tk.lemma_][tk.dep_][ptn].append(ngram)\n",
    "                sents[tk.lemma_][tk.dep_][ptn].append(tk.doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns['discuss']\n",
    "# sents['conj']['V-ed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c7ed0b9ff6d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_parse_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I want to discuss about my life. I rely my ability.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'explacy' is not defined"
     ]
    }
   ],
   "source": [
    "import utils.explacy\n",
    "explacy.print_parse_info(nlp, 'I want to discuss about my life. I rely my ability.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
