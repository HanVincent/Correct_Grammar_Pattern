{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.before_after import *\n",
    "\n",
    "# from pgrule import mapRW\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "from spacy.tokens import Doc\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import utils.explacy # https://github.com/tylerneylon/explacy\n",
    "# explacy.print_parse_info(nlp, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg') # ('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse BNC 所有句子，再取出有 discuss\n",
    "* read all lines\n",
    "* strip() and lower()\n",
    "* filter out target sentences\n",
    "* ** Use list() to keep filter generator **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 load pickle\n",
    "# # Load the data\n",
    "# with open(\"../dataset/bnc_processed.pickle\", \"rb\") as handle:\n",
    "#     doc_bytes, vocab_bytes = pickle.load(handle)\n",
    "    \n",
    "# nlp.vocab.from_bytes(vocab_bytes)\n",
    "# bnc_docs = [Doc(nlp.vocab).from_bytes(b) for b in doc_bytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_WORD = 'discuss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnc_remains = filter(lambda line: any([tk.lemma_ == TARGET_WORD for tk in line]), bnc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(file, reserved=None):\n",
    "    lines = map(lambda line: line.strip().lower(), open(file, 'r', encoding='utf8'))\n",
    "    remains = filter(lambda line: reserved in line, lines) if reserved else lines\n",
    "    return list(remains)\n",
    "\n",
    "bnc_temp = clean_data('../dataset/bnc.txt', TARGET_WORD)\n",
    "bnc_remains = list(map(lambda line: nlp(line), bnc_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 從 EF 中取得有 discuss 的句子，並且 parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = open('../dataset/efcamp/ef.diff.simplize.despace.txt', 'r', encoding='utf8')\n",
    "# ef_all = []\n",
    "\n",
    "ef_temp = clean_data('../dataset/efcamp/ef.diff.simplize.despace.txt', 'discuss')\n",
    "ef_remains = []\n",
    "for index, line in enumerate(ef_temp):\n",
    "    tokens = line.split(' ')\n",
    "    \n",
    "    aft_sent = nlp(' '.join(to_after(tokens)))\n",
    "    bef_sent = nlp(' '.join(to_before(tokens)))\n",
    "    has_edit = any([tk[:2] in ['[-', '{+'] and TARGET_WORD in tokens[i-1] \n",
    "                    for i, tk in enumerate(tokens)]) \n",
    "    \n",
    "    ef_remains.append({\n",
    "        'origin': line,\n",
    "        'bef_sent': bef_sent,\n",
    "        'aft_sent': aft_sent, \n",
    "        'has_edit': has_edit\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生測資"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(filter(lambda x: x['has_edit'], ef_remains))\n",
    "test_data = [(el['bef_sent'], False) if i < 250 else (el['aft_sent'], True) for i, el in enumerate(test_data[:500])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count bigram and trigram \n",
    "* **NO** stemming or other processsing\n",
    "* split only by **space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.syntax import pos_mapping\n",
    "\n",
    "\n",
    "def get_ngram(token):\n",
    "    line = token.doc\n",
    "    return ' '.join([tk.text for tk in line[token.i : token.i + WINDOW_SIZE]]) # discuss _ _\n",
    "    \n",
    "\n",
    "def get_pattern(token):\n",
    "    line = token.doc\n",
    "    \n",
    "    ptns, start_loc = [], token.i\n",
    "    for i in range(WINDOW_SIZE):\n",
    "        if start_loc < len(line):\n",
    "            is_NC, next_start = is_noun_chunk(line[start_loc])\n",
    "            \n",
    "            ### 遇到標點符號先終止\n",
    "            if line[start_loc].is_punct:\n",
    "                break\n",
    "            if line[start_loc].is_quote:\n",
    "                start_loc += 1\n",
    "                \n",
    "            ptns.append('n' if is_NC else pos_mapping(line[start_loc]))\n",
    "            start_loc = next_start\n",
    "            \n",
    "    return ' '.join(ptns)\n",
    "\n",
    "def all_info(parsed_sents):\n",
    "    # TODO: refactor to class\n",
    "    info = {\n",
    "        'ngrams': Counter(),\n",
    "        'patterns': defaultdict(Counter),\n",
    "        'sents': defaultdict(lambda: [])\n",
    "    }\n",
    "    for line in parsed_sents:\n",
    "        for token in line: \n",
    "            if token.lemma_ == TARGET_WORD:\n",
    "                ngram = get_ngram(token)\n",
    "                ptn = get_pattern(token)\n",
    "\n",
    "                info['ngrams'][ngram] += 1\n",
    "                info['patterns'][is_past_passive(token)][ptn] += 1\n",
    "                info['sents'][ptn].append(line.text)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_freq(counts):\n",
    "    values = list(counts.values())\n",
    "    total, avg, std = np.sum(values), np.mean(values), np.std(values)\n",
    "    print(\"Total: {}, Avg: {}, Std: {}\".format(total, avg, std))\n",
    "\n",
    "    return dict([(ngram, count) for ngram, count in counts.items() \n",
    "                 if count > avg + std])\n",
    "\n",
    "def high(info):\n",
    "    return {\n",
    "        'ngrams': get_high_freq(info['ngrams']),\n",
    "        'patterns': {\n",
    "            True: get_high_freq(info['patterns'][True]),\n",
    "            False: get_high_freq(info['patterns'][False])\n",
    "        }\n",
    "    }\n",
    "\n",
    "def sort_dict(counts):\n",
    "    return sorted(counts.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以 BNC 資料統計出正確 patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "bnc = all_info(bnc_remains)\n",
    "high_bnc = high(bnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict(high_bnc['patterns'][False]) # 這裡的 VBN 為完成式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnc['sents']['V']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以 EF 資料統計正確和錯誤的 patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before EF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_bef_sents = map(lambda obj: obj['bef_sent'] , ef_remains)\n",
    "ef_bef = all_info(ef_bef_sents)\n",
    "high_ef_bef = high(ef_bef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict(high_ef_bef['patterns'][False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After EF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_aft_sents = map(lambda obj: obj['aft_sent'] , ef_remains)\n",
    "ef_aft = all_info(ef_aft_sents)\n",
    "high_ef_aft = high(ef_aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict(high_ef_aft['patterns'][False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit EF (Temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_edit = {\n",
    "    'ngrams': defaultdict(Counter), \n",
    "    'patterns': defaultdict(lambda: defaultdict(lambda: Counter())), \n",
    "    'sents': defaultdict(lambda: defaultdict(lambda: []))\n",
    "}\n",
    "ef_right = {\n",
    "    'ngrams': Counter(), \n",
    "    'patterns': defaultdict(Counter), \n",
    "    'sents': defaultdict(lambda: [])\n",
    "}\n",
    "\n",
    "ef_edit_sents = filter(lambda obj: obj['has_edit'], ef_remains)\n",
    "for obj in ef_edit_sents:\n",
    "    origin, bef_sent, aft_sent = obj['origin'], obj['bef_sent'], obj['aft_sent']\n",
    "    \n",
    "    bef_ptn, aft_ptn = None, None\n",
    "    for token in bef_sent: \n",
    "        if token.lemma_ == TARGET_WORD:\n",
    "            is_pp = is_past_passive(token)\n",
    "            bef_ngram = get_ngram(token)\n",
    "            bef_ptn = get_pattern(token)\n",
    "\n",
    "    for token in aft_sent:\n",
    "        if token.lemma_ == TARGET_WORD:\n",
    "            aft_ngram = get_ngram(token)\n",
    "            aft_ptn = get_pattern(token)\n",
    "\n",
    "    ### 先不要考慮單獨 before or after\n",
    "    if bef_ptn and aft_ptn: # 前後都存在 target word\n",
    "        if bef_ptn != aft_ptn: # 前後有更改\n",
    "            ef_edit['ngrams'][bef_ngram][aft_ngram] += 1\n",
    "            ef_edit['patterns'][is_pp][bef_ptn][aft_ptn] += 1\n",
    "            ef_edit['sents'][bef_ptn][aft_ptn].append(origin)\n",
    "            \n",
    "        elif bef_ptn == aft_ptn: # 前後 ptn 一樣\n",
    "            ef_right['ngrams'][aft_ngram] += 1\n",
    "            ef_right['patterns'][is_pp][aft_ptn] += 1\n",
    "            ef_right['sents'][aft_ptn].append(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_ef_edit = {\n",
    "    'patterns': {\n",
    "        True: get_high_freq(dict([(ptn, sum(ctn.values())) for ptn, ctn in ef_edit['patterns'][True].items()])),\n",
    "        False: get_high_freq(dict([(ptn, sum(ctn.values())) for ptn, ctn in ef_edit['patterns'][False].items()]))    \n",
    "    }\n",
    "}\n",
    "high_ef_edit['patterns'][False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef_edit['patterns'][False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(table):\n",
    "    return dict([(ptn, (i+1, ctn)) for i, (ptn, ctn) in enumerate(sort_dict(table))])\n",
    "        \n",
    "def ranking(bnc_table, ef_table):\n",
    "    bnc_rank = transform(bnc_table)\n",
    "    # ef_rank = transform(ef_table)\n",
    "    \n",
    "    print(\"Pattern\\tRank(EF->BNC)\\tRatio(EF/BNC)\")\n",
    "    for i, (ptn, ctn) in enumerate(sort_dict(ef_table)):\n",
    "        if ptn in bnc_rank:\n",
    "            print(\"{}\\t{}->{}\\t{}\".format(ptn, i+1, bnc_rank[ptn][0], bnc_rank[ptn][1]/ctn))\n",
    "        else:\n",
    "            not_exist.append(ptn)\n",
    "            # print(\"{} NOT EXIST in BNC\".format(ptn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_PP = False\n",
    "not_exist = []\n",
    "ranking(high_bnc['patterns'][is_PP], high_ef_bef['patterns'][is_PP])\n",
    "print(not_exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnc['patterns'][False]\n",
    "# temp = nlp(bnc['sents']['V V V'][0])[6]\n",
    "# temp.is_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 觀察區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_PP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict(high_bnc['patterns'][is_PP]) # 這裡的 VBN 為完成式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc['sents']['V N of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict(high_ef_bef['patterns'][is_PP]) # 這裡的 VBN 為完成式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict(high_ef_aft['patterns'][is_PP]) # 這裡的 VBN 為完成式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_exist(token):\n",
    "    ptn = get_pattern(token)\n",
    "    is_pp = is_past_passive(token)\n",
    "    return ptn in high_bnc['patterns'][is_pp]\n",
    "\n",
    "y_test = [is_exist(token) == answer for line, answer in test_data for token in line if token.lemma_ == TARGET_WORD]\n",
    "print(sum(y_test) / len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
